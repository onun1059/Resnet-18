{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NyI37cNqLKgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e93c7fb-89a2-41fd-b5b4-b6364a70d1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 1. 구글드라이브와 colab 연동  \n",
        "from google.colab import drive  # 구글드라이브와 연동하기 위한 라이브러리\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dnrcJU7sLKdU"
      },
      "outputs": [],
      "source": [
        "# 2. 라이브러리 import\n",
        "# '''\n",
        "# pytorch, torchvision, matplotlib, numpy, tqdm, time 라이브러리가 요구된다.\n",
        "# '''\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x0jpKuWALKaW"
      },
      "outputs": [],
      "source": [
        "# 3. 하이퍼 파라미터 설정\n",
        "EPOCH = 150\n",
        "batch_size = 128\n",
        "learning_rate = .1\n",
        "momentum = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MpteSvGGLKTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99356ce-ae0b-49c4-b7e1-0072402edec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0 is available\n"
          ]
        }
      ],
      "source": [
        "# 4. torch에서 사용할 gpu 설정\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'{device} is available')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rEh12vuwLJ_S"
      },
      "outputs": [],
      "source": [
        "# 5. Classification할 Class list 지정\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "axEip7i2OlNC"
      },
      "outputs": [],
      "source": [
        "# 6. 이미지 전처리\n",
        "transform_train = transforms.Compose([   \n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4gR0kAkOlES",
        "outputId": "53fbd63b-46d6-4932-8cf8-518d06c4d7b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# 7. (수정불필요) Dataset 저장 및 Tensor 형태로 변환\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0IhZNAvwJ4bB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exGnfkjaKBOO",
        "outputId": "51948b7b-6890-49c4-ff31-9fda6a3a01c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 0\n"
          ]
        }
      ],
      "source": [
        "# 모의 test 시 train_ration에 0.99 할당 / test 시 valid 필요 x-> train_ratio 1\n",
        "train_ratio = 1\n",
        "train_size = int(len(trainset) * train_ratio)\n",
        "valid_size = len(trainset) - train_size\n",
        "trainset, validset = random_split(trainset, [train_size, valid_size])\n",
        "print(len(trainset), len(validset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ecd1SjLnKZBX"
      },
      "outputs": [],
      "source": [
        "# 모의 test시 validloader #제거\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "# validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ScCPjwe5Ok7Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "\n",
        "# ResNet18을 위해 최대한 간단히 수정한 BasicBlock 클래스\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # 3x3 필터를 사용 (너비와 높이를 줄일 때는 stride 값 조절)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
        "       \n",
        "        # 3x3 필터를 사용 (패딩을 1만큼 주기 때문에 너비와 높이가 동일)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
        "\n",
        "        self.shortcut = nn.Sequential() # identity인 경우\n",
        "        if stride != 1: # stride가 1이 아니라면, identity mapping이 아닌 경우\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x) # (핵심) skip connection\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet 클래스 정의\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # 64개의 3x3 필터(filter)를 사용\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes # 다음 레이어를 위해 채널 수 변경\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out) # 출력: [batch_size, 512, 4, 4]\n",
        "        out = F.avg_pool2d(out, 4) # 출력: [batch_size, 512, 1, 1]\n",
        "        out = out.view(out.size(0), -1) # 출력: [batch_size, 512]\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet18 함수 정의\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJHmJTF7OkyZ",
        "outputId": "625ea637-ebf1-4ee6-dbd8-5a9783cc7453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [128, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2          [128, 64, 32, 32]             128\n",
            "            Conv2d-3          [128, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-4          [128, 64, 32, 32]             128\n",
            "            Conv2d-5          [128, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6          [128, 64, 32, 32]             128\n",
            "        BasicBlock-7          [128, 64, 32, 32]               0\n",
            "            Conv2d-8          [128, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-9          [128, 64, 32, 32]             128\n",
            "           Conv2d-10          [128, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11          [128, 64, 32, 32]             128\n",
            "       BasicBlock-12          [128, 64, 32, 32]               0\n",
            "           Conv2d-13         [128, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-14         [128, 128, 16, 16]             256\n",
            "           Conv2d-15         [128, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-16         [128, 128, 16, 16]             256\n",
            "           Conv2d-17         [128, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-18         [128, 128, 16, 16]             256\n",
            "       BasicBlock-19         [128, 128, 16, 16]               0\n",
            "           Conv2d-20         [128, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-21         [128, 128, 16, 16]             256\n",
            "           Conv2d-22         [128, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-23         [128, 128, 16, 16]             256\n",
            "       BasicBlock-24         [128, 128, 16, 16]               0\n",
            "           Conv2d-25           [128, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-26           [128, 256, 8, 8]             512\n",
            "           Conv2d-27           [128, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-28           [128, 256, 8, 8]             512\n",
            "           Conv2d-29           [128, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-30           [128, 256, 8, 8]             512\n",
            "       BasicBlock-31           [128, 256, 8, 8]               0\n",
            "           Conv2d-32           [128, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-33           [128, 256, 8, 8]             512\n",
            "           Conv2d-34           [128, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-35           [128, 256, 8, 8]             512\n",
            "       BasicBlock-36           [128, 256, 8, 8]               0\n",
            "           Conv2d-37           [128, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-38           [128, 512, 4, 4]           1,024\n",
            "           Conv2d-39           [128, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-40           [128, 512, 4, 4]           1,024\n",
            "           Conv2d-41           [128, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-42           [128, 512, 4, 4]           1,024\n",
            "       BasicBlock-43           [128, 512, 4, 4]               0\n",
            "           Conv2d-44           [128, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-45           [128, 512, 4, 4]           1,024\n",
            "           Conv2d-46           [128, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-47           [128, 512, 4, 4]           1,024\n",
            "       BasicBlock-48           [128, 512, 4, 4]               0\n",
            "           Linear-49                  [128, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,173,962\n",
            "Trainable params: 11,173,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.50\n",
            "Forward/backward pass size (MB): 1440.01\n",
            "Params size (MB): 42.63\n",
            "Estimated Total Size (MB): 1484.14\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 9. Network 선언 및 device 위에 올리기\n",
        "from torchsummary import summary\n",
        "net = ResNet18().to(device)\n",
        "summary(net, (3, 32, 32), batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "s63iaDE7acnk"
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 50:\n",
        "        lr /= 10\n",
        "    if epoch >= 100:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JVglFwsSJwnc"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        n_total, n_correct = 0, 0\n",
        "        running_loss = 0.0\n",
        "        for img, labels in dataloader:\n",
        "            out = model(img.to(device))\n",
        "            _, y_pred = torch.max(out.data, 1)\n",
        "            n_correct += (y_pred==labels.to(device)).sum().item()\n",
        "            n_total += img.size(0)\n",
        "            running_loss += criterion(out, labels.to(device)).item()\n",
        "        val_acc = n_correct/n_total\n",
        "    return val_acc, running_loss/len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZyoA-vNE5lo_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "mixup_alpha = 1.0\n",
        "\n",
        "def mixup_data(x, y):\n",
        "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "    def forward(self, y, targets, smoothing=0.1):\n",
        "        confidence = 1. - smoothing\n",
        "        log_probs = F.log_softmax(y, dim=-1) # 예측 확률 계산\n",
        "        true_probs = torch.zeros_like(log_probs)\n",
        "        true_probs.fill_(smoothing / (y.shape[1] - 1))\n",
        "        # 정답 인덱스의 정답 확률을 confidence로 변경\n",
        "        true_probs.scatter_(1, targets.data.unsqueeze(1), confidence)\n",
        "        # negative log likelihood \n",
        "        return torch.mean(torch.sum(true_probs * -log_probs, dim=-1)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "AaA1WMk0uo4T"
      },
      "outputs": [],
      "source": [
        "net = ResNet18()\n",
        "net = net.to(device)\n",
        "\n",
        "# 이전 기록에서 이어서 학습을 원하면 주석을 해제할 것.\n",
        "#net.load_state_dict(torch.load('/usr/local/lib/python3.10/dist-packages/google/colab/drive.py'))\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "optimizer = optim.SGD(net.parameters(), \n",
        "            lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZyOITACuSqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6553e3f-26a8-48a6-bd7d-f8f9dab5cf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 168/391 [00:24<00:34,  6.47it/s]"
          ]
        }
      ],
      "source": [
        "# 12. Training\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "accuracies = []\n",
        "\n",
        "n = len(trainloader)\n",
        "\n",
        "for epoch in range(EPOCH): \n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  running_loss = 0.0\n",
        "  start = time.time()\n",
        "\n",
        "\n",
        "  for data in tqdm(trainloader):\n",
        "\n",
        "    inputs, targets = data[0].to(device), data[1].to(device) # 배치 데이터\n",
        "    inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "    \n",
        "    optimizer.zero_grad() # 배치마다 optimizer 초기화\n",
        "    outputs = net(inputs)\n",
        "    \n",
        "    loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam) \n",
        "    # Trainging-크로스 엔트로피 손실함수 계산 \n",
        "\n",
        "    loss.backward() # backpropagation\n",
        "    # import pdb; pdb.set_trace()\n",
        "    optimizer.step() # 가중치 최적화\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "############ valid: 모의 test시 실행\n",
        "#   valid_acc, valid_loss = evaluate(net, validloader, criterion)\n",
        "  train_loss = running_loss / n\n",
        "  train_losses.append(train_loss)\n",
        "#   valid_losses.append(valid_loss)\n",
        "#   accuracies.append(valid_acc)\n",
        "  torch.save(net.state_dict() ,'/usr/local/lib/python3.10/dist-packages/google/colab/drive.py')\n",
        "\n",
        "  #수정# loss_.append(running_loss / n)\n",
        "  #수정# print('[%d] loss: %.3f' %(epoch + 1, running_loss / len(trainloader)))\n",
        "  print('[%d] loss: %.3f' %(epoch + 1, running_loss / n))\n",
        "  print(\"epoch time :\", time.time()-start)\n",
        "#   print(\"valid accuracy : \", round다음(accuracies[-1], 3))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV__XLonLrml"
      },
      "outputs": [],
      "source": [
        "#valid <-> train loss 출력\n",
        "plt.figure(figsize=(14,12))   \n",
        "plt.title(\"train vs valid loss\")\n",
        "plt.plot(train_losses, label=\"TRAIN\")\n",
        "plt.plot(valid_losses, label=\"VALID\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL60OV8zVOf-"
      },
      "outputs": [],
      "source": [
        "#valid <-> train accuracy 출력\n",
        "plt.figure(figsize=(14,12))    \n",
        "plt.title(\"train vs valid accuracy\")\n",
        "plt.plot(accuracies, label=\"VALID\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrIiJgfWO3Xd"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "net = ResNet18()\n",
        "net.to(device)\n",
        "# !!주의!! weight 저장경로는 상황에 맞게 수정\n",
        "net.load_state_dict(torch.load('/usr/local/lib/python3.10/dist-packages/google/colab/drive.py'))\n",
        "\n",
        "# (수정불가) Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "correct_pred = {classname: 0 for classname in classes}  #dictionary 생성\n",
        "total_pred = {classname: 0 for classname in classes}    #dictionary 생성\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad(): # autograd를 끔으로써 메모리 사용량을 줄이고 연산 속도를 높일 수 있다.\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0) # labels.size = batch_size\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                   accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3UMcDGqO3DY"
      },
      "outputs": [],
      "source": [
        "#Inference\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "    img = img/2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "#수정# images, labels = dataiter.next()\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# GroundTruth display\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))\n",
        "\n",
        "# Predicted display\n",
        "net = ResNet18()\n",
        "net.to(device)\n",
        "# !!주의!! weight 저장경로는 상황에 맞게 수정\n",
        "net.load_state_dict(torch.load('/usr/local/lib/python3.10/dist-packages/google/colab/drive.py'))\n",
        "\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(8)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}